{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tgog/.conda/envs/my-rdkit-env/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc45e04140b4300a22471d956f62d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_ColormakerRegistry()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "9\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "glug glug\n",
      "reset called\n",
      "[118.12815655611193, 63.310666530774434, 173.2125075981846, 48.72733628320435, 168.46036720948098, 104.40792978250536, -178.33034393068147, -177.28834591074718]\n",
      "Data(edge_attr=[168, 9], edge_index=[2, 168], pos=[46, 3], x=[46, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ab30ebf7050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import multiprocessing\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.transforms import Distance\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "from utils import *\n",
    "import envs\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from deep_rl import *\n",
    "\n",
    "from deep_rl.component.envs import DummyVecEnv, make_env\n",
    "\n",
    "import envs\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pointnet import SAModule, GlobalSAModule, MLP\n",
    "# from torch_geometric.nn import knn_interpolate\n",
    "# from torch_geometric.utils import intersection_and_union as i_and_u\n",
    "\n",
    "# class FPModule(torch.nn.Module):\n",
    "#     def __init__(self, k, nn):\n",
    "#         super(FPModule, self).__init__()\n",
    "#         self.k = k\n",
    "#         self.nn = nn\n",
    "\n",
    "#     def forward(self, x, pos, batch, x_skip, pos_skip, batch_skip):\n",
    "#         x = knn_interpolate(x, pos, pos_skip, batch, batch_skip, k=self.k)\n",
    "#         if x_skip is not None:\n",
    "#             x = torch.cat([x, x_skip], dim=1)\n",
    "#         x = self.nn(x)\n",
    "#         return x, pos_skip, batch_skip\n",
    "\n",
    "\n",
    "# class ActorPointNet(torch.nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.sa1_module = SAModule(0.2, 0.2, MLP([3 + 3, 64, 64, 128]))\n",
    "#         self.sa2_module = SAModule(0.25, 0.4, MLP([128 + 3, 128, 128, 256]))\n",
    "#         self.sa3_module = GlobalSAModule(MLP([256 + 3, 256, 512, 1024]))\n",
    "\n",
    "#         self.fp3_module = FPModule(1, MLP([1024 + 256, 256, 256]))\n",
    "#         self.fp2_module = FPModule(3, MLP([256 + 128, 256, 128]))\n",
    "#         self.fp1_module = FPModule(3, MLP([128, 128, 128, 128]))\n",
    "\n",
    "#         self.lin1 = torch.nn.Linear(128, 128)\n",
    "#         self.lin2 = torch.nn.Linear(128, 128)\n",
    "#         self.lin3 = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         sa0_out = (data.x, data.pos, data.batch)\n",
    "#         sa1_out = self.sa1_module(*sa0_out)\n",
    "#         sa2_out = self.sa2_module(*sa1_out)\n",
    "#         sa3_out = self.sa3_module(*sa2_out)\n",
    "\n",
    "#         fp3_out = self.fp3_module(*sa3_out, *sa2_out)\n",
    "#         fp2_out = self.fp2_module(*fp3_out, *sa1_out)\n",
    "#         x, _, _ = self.fp1_module(*fp2_out, *sa0_out)\n",
    "\n",
    "#         x = F.relu(self.lin1(x))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.lin2(x)\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.lin3(x)\n",
    "#         return F.log_softmax(x, dim=-1)\n",
    "    \n",
    "# class CriticPointNet(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CriticPointNet, self).__init__()\n",
    "\n",
    "#         self.sa1_module = SAModule(0.5, 0.2, MLP([3, 64, 64, 128]))\n",
    "#         self.sa2_module = SAModule(0.25, 0.4, MLP([128 + 3, 128, 128, 256]))\n",
    "#         self.sa3_module = GlobalSAModule(MLP([256 + 3, 256, 512, 1024]))\n",
    "\n",
    "#         self.lin1 = Lin(1024, 512)\n",
    "#         self.lin2 = Lin(512, 256)\n",
    "#         self.lin3 = Lin(256, 1)\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         sa0_out = (data.x, data.pos, data.batch)\n",
    "#         sa1_out = self.sa1_module(*sa0_out)\n",
    "#         sa2_out = self.sa2_module(*sa1_out)\n",
    "#         sa3_out = self.sa3_module(*sa2_out)\n",
    "#         x, pos, batch = sa3_out\n",
    "\n",
    "#         x = F.relu(self.lin1(x))\n",
    "#         x = F.relu(self.lin2(x))\n",
    "#         x = self.lin3(x)\n",
    "#         return x\n",
    "    \n",
    "# class PointTorsionNet(torch.nn.Module):\n",
    "#         def __init__(self):\n",
    "#             super(PointTorsionNet, self).__init__()\n",
    "\n",
    "#             self.actor = ActorPointNet(6)\n",
    "#             self.critic = CriticPointNet()\n",
    "#         def forward(self, obs, states=None):\n",
    "#             if states:\n",
    "#                 hp, cp, hv, cv = states\n",
    "#                 policy_states = (hp, cp)\n",
    "#                 value_states = (hv, cv)\n",
    "#             else:\n",
    "#                 policy_states = None\n",
    "#                 value_states = None\n",
    "\n",
    "#             logits, (hp, cp) = self.actor(obs, policy_states)\n",
    "#             v, (hv, cv) = self.critic(obs, value_states)\n",
    "\n",
    "#             dist = torch.distributions.Categorical(logits=logits)\n",
    "#             action = dist.sample().cuda()\n",
    "#             log_prob = dist.log_prob(action).unsqueeze(0).cuda()\n",
    "#             entropy = dist.entropy().unsqueeze(0).cuda()\n",
    "\n",
    "#             prediction = {\n",
    "#                 'a': action,\n",
    "#                 'log_pi_a': log_prob,\n",
    "#                 'ent': entropy,\n",
    "#                 'v': v,\n",
    "#             }\n",
    "\n",
    "#             return prediction, (hp, cp, hv, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticBatchNet(torch.nn.Module):\n",
    "    def __init__(self, action_dim, dim, edge_dim):\n",
    "        super(CriticBatchNet, self).__init__()\n",
    "        num_features = 3\n",
    "        self.lin0 = torch.nn.Linear(num_features, dim)\n",
    "        func_ag = nn.Sequential(nn.Linear(edge_dim, dim), nn.ReLU(), nn.Linear(dim, dim * dim))\n",
    "        self.conv = gnn.NNConv(dim, dim, func_ag, aggr='mean')\n",
    "        self.gru = nn.GRU(dim, dim)\n",
    "\n",
    "        self.set2set = gnn.Set2Set(dim, processing_steps=6)\n",
    "        self.lin1 = torch.nn.Linear(dim, dim)\n",
    "        self.lin3 = torch.nn.Linear(dim, 1)\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.dim = dim\n",
    "\n",
    "        self.memory = nn.LSTM(2*dim, dim)\n",
    "\n",
    "    def forward(self, obs, states=None):\n",
    "        data, nonring, nrbidx, torsion_list_sizes = obs\n",
    "        data.to(torch.device(0))\n",
    "\n",
    "        if states:\n",
    "            hx, cx = states\n",
    "        else:\n",
    "            hx = Variable(torch.zeros(1, data.num_graphs, self.dim)).cuda()\n",
    "            cx = Variable(torch.zeros(1, data.num_graphs, self.dim)).cuda()\n",
    "\n",
    "        out = F.relu(self.lin0(data.x.cuda()))\n",
    "        h = out.unsqueeze(0)\n",
    "\n",
    "        for i in range(6):\n",
    "            m = F.relu(self.conv(out, data.edge_index, data.edge_attr))\n",
    "            out, h = self.gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "        pool = self.set2set(out, data.batch)\n",
    "        lstm_out, (hx, cx) = self.memory(pool.view(1,data.num_graphs,-1), (hx, cx))\n",
    "        out = F.relu(self.lin1(lstm_out))\n",
    "        v = self.lin3(out)\n",
    "\n",
    "        return v, (hx, cx)\n",
    "\n",
    "class ActorBatchNet(torch.nn.Module):\n",
    "    def __init__(self, action_dim, dim, edge_dim):\n",
    "        super(ActorBatchNet, self).__init__()\n",
    "        num_features = 3\n",
    "        self.lin0 = torch.nn.Linear(num_features, dim)\n",
    "        func_ag = nn.Sequential(nn.Linear(edge_dim, dim), nn.ReLU(), nn.Linear(dim, dim * dim))\n",
    "        self.conv = gnn.NNConv(dim, dim, func_ag, aggr='mean')\n",
    "        self.gru = nn.GRU(dim, dim)\n",
    "\n",
    "        self.set2set = gnn.Set2Set(dim, processing_steps=6)\n",
    "        self.lin1 = torch.nn.Linear(5 * dim, dim)\n",
    "        self.lin2 = torch.nn.Linear(dim, action_dim)\n",
    "\n",
    "        self.memory = nn.LSTM(2*dim, dim)\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, obs, states=None):\n",
    "        data, nonring, nrbidx, torsion_list_sizes = obs\n",
    "        data.to(torch.device(0))\n",
    "\n",
    "        if states:\n",
    "            hx, cx = states\n",
    "        else:\n",
    "            hx = Variable(torch.zeros(1, data.num_graphs, self.dim)).cuda()\n",
    "            cx = Variable(torch.zeros(1, data.num_graphs, self.dim)).cuda()\n",
    "\n",
    "        out = F.relu(self.lin0(data.x.cuda()))\n",
    "        h = out.unsqueeze(0)\n",
    "\n",
    "        for i in range(6):\n",
    "            m = F.relu(self.conv(out, data.edge_index, data.edge_attr))\n",
    "            out, h = self.gru(m.unsqueeze(0), h)\n",
    "            out = out.squeeze(0)\n",
    "        pool = self.set2set(out, data.batch)\n",
    "        lstm_out, (hx, cx) = self.memory(pool.view(1,data.num_graphs,-1), (hx, cx))\n",
    "\n",
    "        print(lstm_out.shape)\n",
    "        lstm_out = torch.index_select(\n",
    "            lstm_out,\n",
    "            dim=1,\n",
    "            index=nrbidx\n",
    "        )\n",
    "        print(lstm_out.shape)\n",
    "        \n",
    "        out = torch.index_select(\n",
    "            out,\n",
    "            dim=0,\n",
    "            index=nonring.view(-1)\n",
    "        ).view(4, -1, self.dim)\n",
    "        print(out.shape)\n",
    "\n",
    "        out = torch.cat([lstm_out,out],0)   #5, num_torsions, self.dim\n",
    "        out = out.permute(2,1,0).reshape(-1, 5*self.dim) #num_torsions, 5*self.dim\n",
    "        out = F.relu(self.lin1(out))\n",
    "        out = self.lin2(out)\n",
    "\n",
    "        logit = out.split(torsion_list_sizes)\n",
    "        logit = torch.nn.utils.rnn.pad_sequence(logit).permute(1,0,2)\n",
    "\n",
    "        return logit, (hx, cx)\n",
    "\n",
    "class RTGNBatch(torch.nn.Module):\n",
    "    def __init__(self, action_dim, dim, edge_dim=7, point_dim=3):\n",
    "        super(RTGNBatch, self).__init__()\n",
    "        num_features = point_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.dim = dim\n",
    "\n",
    "        self.actor = ActorBatchNet(action_dim, dim, edge_dim=edge_dim)\n",
    "        self.critic = CriticBatchNet(action_dim, dim, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, obs, states=None, action=None):\n",
    "        data_list = []\n",
    "        nr_list = []\n",
    "        for b, nr in obs:\n",
    "            data_list += b.to_data_list()\n",
    "            nr_list.append(torch.LongTensor(nr).cuda())\n",
    "\n",
    "        b = Batch.from_data_list(data_list)\n",
    "        so_far = 0\n",
    "        torsion_batch_idx = []\n",
    "        torsion_list_sizes = []\n",
    "\n",
    "        for i in range(b.num_graphs):\n",
    "            trues = (b.batch == i).view(1, -1)\n",
    "            nr_list[i] += so_far\n",
    "            so_far += int((b.batch == i).sum())\n",
    "            torsion_batch_idx.extend([i]*int(nr_list[i].shape[0]))\n",
    "            torsion_list_sizes += [nr_list[i].shape[0]]\n",
    "\n",
    "        nrs = torch.cat(nr_list)\n",
    "        torsion_batch_idx = torch.LongTensor(torsion_batch_idx).cuda()\n",
    "        obs = (b, nrs, torsion_batch_idx, torsion_list_sizes)\n",
    "\n",
    "        if states:\n",
    "            hp, cp, hv, cv = states\n",
    "            policy_states = (hp, cp)\n",
    "            value_states = (hv, cv)\n",
    "        else:\n",
    "            policy_states = None\n",
    "            value_states = None\n",
    "\n",
    "        logits, (hp, cp) = self.actor(obs, policy_states)\n",
    "        v, (hv, cv) = self.critic(obs, value_states)\n",
    "\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        action = dist.sample().cuda()\n",
    "        log_prob = dist.log_prob(action).unsqueeze(0).cuda()\n",
    "        entropy = dist.entropy().unsqueeze(0).cuda()\n",
    "\n",
    "        prediction = {\n",
    "            'a': action,\n",
    "            'log_pi_a': log_prob,\n",
    "            'ent': entropy,\n",
    "            'v': v,\n",
    "        }\n",
    "\n",
    "        return prediction, (hp, cp, hv, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#message passing\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class CriticTransformer(torch.nn.Module):\n",
    "    def __init__(self, action_dim, dim):\n",
    "        super(CriticTransformer, self).__init__()\n",
    "        num_features = 3\n",
    "        self.lin0 = torch.nn.Linear(num_features, dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "        self.set2set = gnn.Set2Set(dim, processing_steps=6)\n",
    "        self.lin1 = torch.nn.Linear(dim, dim)\n",
    "        self.lin3 = torch.nn.Linear(dim, 1)\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.dim = dim\n",
    "\n",
    "        self.memory = nn.LSTM(2*dim, dim)\n",
    "\n",
    "    def forward(self, obs, states=None):\n",
    "        data, nonring, nrbidx, torsion_list_sizes = obs\n",
    "\n",
    "        if states:\n",
    "            hx, cx = states\n",
    "        else:\n",
    "            hx = Variable(torch.zeros(1, data.shape[0], self.dim)).cuda()\n",
    "            cx = Variable(torch.zeros(1, data.shape[0], self.dim)).cuda()\n",
    "\n",
    "        out = F.relu(self.lin0(data))\n",
    "        out = self.transformer_encoder(out)\n",
    "        \n",
    "        num_atoms = out.shape[1]\n",
    "        batch_len = out.shape[0]\n",
    "        bidxs = []\n",
    "        \n",
    "        for i in range(batch_len):\n",
    "            bidxs.append(torch.ones([num_atoms], dtype=torch.int32) * i)\n",
    "            \n",
    "        bidxs = torch.cat(bidxs).long().cuda()\n",
    "        pool_in = torch.flatten(out, start_dim=0, end_dim=1)  \n",
    "\n",
    "    \n",
    "        pool = self.set2set(pool_in, bidxs)\n",
    "        lstm_out, (hx, cx) = self.memory(pool.view(1,data.shape[0],-1), (hx, cx))\n",
    "        \n",
    "        out = F.relu(self.lin1(lstm_out))\n",
    "        v = self.lin3(out)\n",
    "\n",
    "        return v, (hx, cx)\n",
    "\n",
    "class ActorTransformer(torch.nn.Module):\n",
    "    def __init__(self, action_dim, dim):\n",
    "        super(ActorTransformer, self).__init__()\n",
    "        num_features = 3\n",
    "\n",
    "        self.lin0 = torch.nn.Linear(num_features, dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "        self.set2set = gnn.Set2Set(dim, processing_steps=6)\n",
    "        self.lin1 = torch.nn.Linear(5 * dim, dim)\n",
    "        self.lin2 = torch.nn.Linear(dim, action_dim)\n",
    "\n",
    "        self.memory = nn.LSTM(2*dim, dim)\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.dim = dim\n",
    "\n",
    "\n",
    "    def forward(self, obs, states=None):\n",
    "        data, nonring, nrbidx, torsion_list_sizes = obs\n",
    "\n",
    "        \n",
    "        if states:\n",
    "            hx, cx = states\n",
    "        else:\n",
    "            hx = Variable(torch.zeros(1, data.shape[0], self.dim)).cuda()\n",
    "            cx = Variable(torch.zeros(1, data.shape[0], self.dim)).cuda()            \n",
    "            \n",
    "        out = F.relu(self.lin0(data))\n",
    "        out = self.transformer_encoder(out)        \n",
    "        \n",
    "        num_atoms = out.shape[1]\n",
    "        batch_len = out.shape[0]\n",
    "        bidxs = []\n",
    "        \n",
    "        for i in range(batch_len):\n",
    "            bidxs.append(torch.ones([num_atoms], dtype=torch.int32) * i)\n",
    "            \n",
    "        bidxs = torch.cat(bidxs).long().cuda()\n",
    "        pool_in = torch.flatten(out, start_dim=0, end_dim=1)  \n",
    "\n",
    "    \n",
    "        pool = self.set2set(pool_in, bidxs)\n",
    "        lstm_out, (hx, cx) = self.memory(pool.view(1,data.shape[0],-1), (hx, cx))\n",
    "\n",
    "        lstm_out = torch.index_select(\n",
    "            lstm_out,\n",
    "            dim=1,\n",
    "            index=nrbidx\n",
    "        )  \n",
    "        \n",
    "        out = out.view(1, num_atoms * batch_len, self.dim)\n",
    "        out = torch.index_select(\n",
    "            out,\n",
    "            dim=1,\n",
    "            index=nonring.view(-1)\n",
    "        )\n",
    "    \n",
    "        out = out.view(4, -1, self.dim)       \n",
    "\n",
    "        out = torch.cat([lstm_out,out],0)   #5, num_torsions, self.dim\n",
    "        out = out.permute(2,1,0).reshape(-1, 5*self.dim) #num_torsions, 5*self.dim\n",
    "        out = F.relu(self.lin1(out))\n",
    "        out = self.lin2(out)\n",
    "\n",
    "        logit = out.split(torsion_list_sizes)\n",
    "        logit = torch.nn.utils.rnn.pad_sequence(logit).permute(1,0,2)\n",
    "        return logit, (hx, cx)\n",
    "    \n",
    "class GraphTransformerBatch(torch.nn.Module):\n",
    "    def __init__(self, action_dim, dim, point_dim=3):\n",
    "        super(GraphTransformerBatch, self).__init__()\n",
    "        num_features = point_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.dim = dim\n",
    "\n",
    "        self.actor = ActorTransformer(action_dim, dim)\n",
    "        self.critic = CriticTransformer(action_dim, dim)\n",
    "\n",
    "    def forward(self, obs, states=None, action=None):\n",
    "        # data_list = []\n",
    "        # nr_list = []\n",
    "        # for b, nr in obs:\n",
    "        #     data_list.append(b.x.cuda())\n",
    "        #     nr_list.append(torch.LongTensor(nr).cuda())\n",
    "\n",
    "        # torsion_batch_idx = []\n",
    "        # torsion_list_sizes = []\n",
    "\n",
    "        # for i in range(len(obs)):\n",
    "        #     trues = (b.batch == i).view(1, -1)\n",
    "        #     nr_list[i] += so_far\n",
    "        #     so_far += int((b.batch == i).sum())\n",
    "        #     torsion_batch_idx.extend([i]*int(nr_list[i].shape[0]))\n",
    "        #     torsion_list_sizes += [nr_list[i].shape[0]]\n",
    "\n",
    "        # nrs = torch.cat(nr_list)\n",
    "        # torsion_batch_idx = torch.LongTensor(torsion_batch_idx).cuda()\n",
    "        # obs = (b, nrs, torsion_batch_idx, torsion_list_sizes)\n",
    "\n",
    "        data_list = []\n",
    "        nr_list = []\n",
    "        for b, nr in obs:\n",
    "            data_list.append(b.x.cuda())\n",
    "            nr_list.append(torch.LongTensor(nr).cuda())\n",
    "\n",
    "        b = torch.nn.utils.rnn.pad_sequence(data_list, batch_first=True)\n",
    "        nrs = torch.nn.utils.rnn.pad_sequence(nr_list, batch_first=True)\n",
    "        \n",
    "#         print(b.shape) #torch.Size([4, 15, 3])\n",
    "        max_atoms = b.shape[1]\n",
    "    \n",
    "#         print(nrs.shape) #4,10,4 batch first\n",
    "\n",
    "        torsion_batch_idx = []\n",
    "        torsion_list_sizes = []\n",
    "        \n",
    "        for i in range(len(obs)):\n",
    "            nrs[i] += max_atoms * i\n",
    "            torsion_batch_idx.extend([i]*int(nr_list[i].shape[0]))\n",
    "            torsion_list_sizes += [nr_list[i].shape[0]]\n",
    "            \n",
    "        torsion_batch_idx = torch.LongTensor(torsion_batch_idx).cuda()\n",
    "        obs = (b, nrs, torsion_batch_idx, torsion_list_sizes)\n",
    "            \n",
    "        if states:\n",
    "            hp, cp, hv, cv = states\n",
    "            policy_states = (hp, cp)\n",
    "            value_states = (hv, cv)\n",
    "        else:\n",
    "            policy_states = None\n",
    "            value_states = None\n",
    "\n",
    "        logits, (hp, cp) = self.actor(obs, states=policy_states)\n",
    " \n",
    "        v, (hv, cv) = self.critic(obs, states=value_states)\n",
    "\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        action = dist.sample().cuda()\n",
    "        log_prob = dist.log_prob(action).unsqueeze(0).cuda()\n",
    "        entropy = dist.entropy().unsqueeze(0).cuda()\n",
    "\n",
    "        prediction = {\n",
    "            'a': action,\n",
    "            'log_pi_a': log_prob,\n",
    "            'ent': entropy,\n",
    "            'v': v,\n",
    "        }\n",
    "\n",
    "        return prediction, (hp, cp, hv, cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphTransformerBatch(\n",
       "  (actor): ActorTransformer(\n",
       "    (lin0): Linear(in_features=3, out_features=128, bias=True)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (set2set): Set2Set(128, 256)\n",
       "    (lin1): Linear(in_features=640, out_features=128, bias=True)\n",
       "    (lin2): Linear(in_features=128, out_features=6, bias=True)\n",
       "    (memory): LSTM(256, 128)\n",
       "  )\n",
       "  (critic): CriticTransformer(\n",
       "    (lin0): Linear(in_features=3, out_features=128, bias=True)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (set2set): Set2Set(128, 256)\n",
       "    (lin1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (lin3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (memory): LSTM(256, 128)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import envs\n",
    "m = GraphTransformerBatch(6, 128)\n",
    "m.to(torch.device('cuda'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed is  55125\n",
      "step time mean nan\n",
      "reset called\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[-179.93808835469966, -179.59844696587746, -175.1584517632214, 87.52936455389786, -59.330298348286064, -57.44536318176025, 163.35002294919693, -138.95710912180365, 170.29548351706333, -141.55473571861535]\n",
      "torch.Size([1, 1, 128])\n",
      "torch.Size([10])\n",
      "torch.Size([1, 10, 128])\n",
      "torch.Size([1, 18, 128])\n",
      "torch.Size([1, 10, 4])\n",
      "torch.Size([1, 40, 128])\n",
      "torch.Size([4, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "task = AdaTask('AllTenTorsionSetDense-v0', seed=random.randint(0,7e4), num_envs=1, single_process=True)\n",
    "x = task.reset()\n",
    "\n",
    "prediction, recurrent_states = m(x)\n",
    "# next_states, rewards, terminals, info = task.step(to_np(prediction['a']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(torch.zeros(1, 1, 128)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import RTGNBatch\n",
    "\n",
    "# model = RTGNBatch(6, 128)\n",
    "# model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c_feature(**kwargs):\n",
    "    generate_tag(kwargs)\n",
    "    kwargs.setdefault('log_level', 0)\n",
    "    config = Config()\n",
    "    config.merge(kwargs)\n",
    "\n",
    "    config.num_workers = 1\n",
    "    single_process = (config.num_workers == 1)\n",
    "    config.task_fn = lambda: AdaTask('OneSet-v0',num_envs=config.num_workers, seed=random.randint(0,1e5), single_process=single_process)\n",
    "    \n",
    "    lr = 7e-5 * np.sqrt(config.num_workers)\n",
    "    \n",
    "    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, lr=lr, alpha=0.99, eps=1e-5) #learning_rate #alpha #epsilon\n",
    "    config.network = model\n",
    "    config.discount = 0.9999 # gamma\n",
    "    config.use_gae = False\n",
    "    config.gae_tau = 0.95\n",
    "    config.value_loss_weight = 0.25 # vf_coef\n",
    "    config.entropy_weight = 0.0005 #ent_coef\n",
    "    config.rollout_length = 5 # n_steps\n",
    "    config.gradient_clip = 0.5 #max_grad_norm\n",
    "    config.max_steps = 5000000\n",
    "    config.save_interval = 10000\n",
    "    config.eval_interval = 2000\n",
    "    config.eval_episodes = 2\n",
    "    config.eval_env = AdaTask('Diff-v0', seed=random.randint(0,7e4))\n",
    "    config.state_normalizer = DummyNormalizer()\n",
    "    \n",
    "    agent = A2CRecurrentEvalAgent(config)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mkdir('log')\n",
    "# mkdir('tf_log')\n",
    "# set_one_thread()\n",
    "# select_device(0)\n",
    "# tag='rtgn_node_memory_new_HP'\n",
    "# agent = a2c_feature(tag=tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_steps(agent)\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# glob.glob('data/*diff*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# reload(graphenvironments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RTGN(6, 64)\n",
    "# model.load_state_dict(torch.load('data/A2CRecurrentAgent-gnn_new_model_basic_carbon-120000.model'))\n",
    "# model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = Chem.MolFromMolFile('lignin_guaiacyl.mol')\n",
    "# m = Chem.AddHs(m)\n",
    "# AllChem.EmbedMultipleConfs(m, numConfs=1, numThreads=0)\n",
    "# res = AllChem.MMFFOptimizeMoleculeConfs(m, numThreads=0)\n",
    "\n",
    "# mol = m\n",
    "\n",
    "# nonring, ring = TorsionFingerprints.CalculateTorsionLists(mol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
